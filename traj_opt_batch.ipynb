{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84810061-0cbe-4494-8450-a30dd615536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import bmesh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "import trimesh\n",
    "import pyrender\n",
    "import bmesh\n",
    "import mathutils\n",
    "import os\n",
    "from util import get_contour,bounds,rotate_2d_np,compute_bbox\n",
    "from util import contour2poly\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point, Polygon,LineString\n",
    "import time\n",
    "\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.prepared import prep\n",
    "from PIL import Image\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6910545c-c0a7-44cf-995b-4cb7db7a5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_object(obj):\n",
    "    # Get the mesh data\n",
    "    mesh = obj.data\n",
    "\n",
    "    # Create a BMesh from the mesh\n",
    "    bm = bmesh.new()\n",
    "    bm.from_mesh(mesh)\n",
    "\n",
    "    # Triangulate all faces\n",
    "    bmesh.ops.triangulate(bm, faces=bm.faces[:])\n",
    "\n",
    "    # Write the bmesh back to the mesh and free bmesh\n",
    "    bm.to_mesh(mesh)\n",
    "    bm.free()\n",
    "\n",
    "def blender_mesh_to_trimesh(obj):\n",
    "    triangulate_object(obj)\n",
    "    mesh = obj.data\n",
    "\n",
    "    vertices_local = [v.co for v in mesh.vertices]\n",
    "    vertices_world = np.array([\n",
    "        (obj.matrix_world @ mathutils.Vector(v)).to_tuple()\n",
    "        for v in vertices_local\n",
    "    ])\n",
    "\n",
    "    faces = np.array([f.vertices[:] for f in mesh.polygons if len(f.vertices) == 3])\n",
    "    tm = trimesh.Trimesh(vertices=vertices_world, faces=faces, process=False)\n",
    "    return tm\n",
    "\n",
    "def mesh_to_xy_polygon(mesh: trimesh.Trimesh) -> Polygon:\n",
    "    # Project vertices onto XY plane by dropping Z coordinate\n",
    "\n",
    "    projected_verts = mesh.vertices[:, :2]  # (N, 2)\n",
    "    \n",
    "    # Get the 2D projected faces as polygons\n",
    "    faces = mesh.faces\n",
    "    face_polys = []\n",
    "    for f in faces:\n",
    "        pts = projected_verts[f]  # 3 points for one triangle\n",
    "        if not trimesh.triangles.area(pts.reshape(1, 3, 2))[0] > 1e-8:\n",
    "            continue  # Skip degenerate triangles\n",
    "        face_polys.append(Polygon(pts))\n",
    "    \n",
    "    # Union all triangle polygons to get a single polygon\n",
    "    return unary_union(face_polys)\n",
    "\n",
    "def get_unique_color(index):\n",
    "    \"\"\"Convert an integer index into a unique RGB color\"\"\"\n",
    "    r = (index & 255)\n",
    "    g = (index >> 8) & 255\n",
    "    b = (index >> 16) & 255\n",
    "    return [r, g, b, 255]  # RGBA\n",
    "\n",
    "def is_valid_point(x, y,prep_room,prep_obstacles):\n",
    "    p = Point(x, y)\n",
    "    # Must be inside room AND not inside any obstacle\n",
    "    return prep_room.contains(p) and not prep_obstacles.intersects(p)\n",
    "\n",
    "def get_nearest_grid_node(x, y, graph):\n",
    "    # Find the node in the graph closest to the requested real-world coordinate\n",
    "    nodes = np.array(graph.nodes)\n",
    "    dist = np.sum((nodes - np.array([x, y]))**2, axis=1)\n",
    "    nearest_idx = np.argmin(dist)\n",
    "    return tuple(nodes[nearest_idx])\n",
    "\n",
    "def compute_path_segment(start_x, start_y, end_x, end_y,G):\n",
    "    start_node = get_nearest_grid_node(start_x, start_y, G)\n",
    "    end_node = get_nearest_grid_node(end_x, end_y, G)\n",
    "    \n",
    "    try:\n",
    "        # Dijkstra (weighted shortest path)\n",
    "        path_nodes = nx.shortest_path(G, source=start_node, target=end_node, weight='weight')\n",
    "        return path_nodes\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path found between ({start_x},{start_y}) and ({end_x},{end_y})\")\n",
    "        return []\n",
    "\n",
    "def get_camera_rotation_matrix(position, yaw, pitch):\n",
    "    \"\"\"\n",
    "    Returns a world matrix for a Blender camera (which points -Z)\n",
    "    oriented according to standard Z-up Yaw/Pitch.\n",
    "    \"\"\"\n",
    "    # 1. Calculate the Look Direction Vector from Yaw/Pitch\n",
    "    # Assuming Yaw starts at X-axis (0) and rotates around Z\n",
    "    # Pitch is elevation from horizon\n",
    "    import math\n",
    "    \n",
    "    # Direction vector (Forward)\n",
    "    # Note: Adjust these sin/cos if your pitch/yaw definition differs!\n",
    "    # Here: Yaw=0 -> +X, Pitch=0 -> Horizon\n",
    "    dir_x = math.cos(yaw) * math.cos(pitch)\n",
    "    dir_y = math.sin(yaw) * math.cos(pitch)\n",
    "    dir_z = math.sin(pitch)\n",
    "\n",
    "    direction = mathutils.Vector((dir_x, dir_y, dir_z))\n",
    "    \n",
    "    # 2. Blender Camera Orientation Correction\n",
    "    # Blender Camera: -Z is Forward, +Y is Up.\n",
    "    # We want -Z to align with 'direction', and +Y to align with World Up (Z)\n",
    "    \n",
    "    rot_quat = direction.to_track_quat('-Z', 'Y')\n",
    "    \n",
    "    # Create the full 4x4 matrix\n",
    "    mat_loc = mathutils.Matrix.Translation(position)\n",
    "    mat_rot = rot_quat.to_matrix().to_4x4()\n",
    "    \n",
    "    return mat_loc @ mat_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9958a4cb-687b-49a1-be4f-68c7116a3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_traj(file_name=\"scene (1).blend\",\n",
    "                room_type=\"living-room_0/0.ceiling\",\n",
    "                OUTPUT_DIR = \"//render_output/\",\n",
    "                FRAME_FILENAME = \"frame_\",\n",
    "                VIDEO_FILENAME = \"trajectory_video.mp4\",\n",
    "                GRID_RES = 0.2,\n",
    "                ROTATION_STEPS = 30,\n",
    "                height=1.5\n",
    "               ):\n",
    "\n",
    "    \n",
    "    exclude_obj= [\"PointLamp\", \"Window\", \"CeilingLight\",\"Door\",\"Carnivore\"]\n",
    "    bpy.ops.wm.open_mainfile(filepath=file_name)\n",
    "    start_all = time.time()\n",
    "    \n",
    "    for ob in bpy.data.objects:\n",
    "        if room_type in ob.name:\n",
    "            x_vals,y_vals = get_contour(ob)\n",
    "    poly_room=contour2poly(x_vals,y_vals)\n",
    "    from shapely.affinity import translate\n",
    "    for ob in bpy.data.objects:\n",
    "        if room_type in ob.name:\n",
    "            offset_x=ob.location.x\n",
    "            offset_y=ob.location.y\n",
    "    \n",
    "    poly_room = translate(poly_room, xoff=offset_x, yoff=offset_y)\n",
    "\n",
    "\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    selected_door_name = None\n",
    "    \n",
    "    for ob in bpy.data.objects:\n",
    "        # Check for \"door\" (case-insensitive)\n",
    "        if \"doorfactory\" in ob.name.lower():\n",
    "            # Get global location\n",
    "            \n",
    "            global_loc = ob.matrix_world.translation\n",
    "            door_point = Point(global_loc.x, global_loc.y)\n",
    "           # print(door_point)\n",
    "            \n",
    "            # Calculate distance to the room boundary\n",
    "            dist = poly_room.boundary.distance(door_point)\n",
    "            \n",
    "            # Update if this is the closest door found so far\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                selected_door_name = ob.name\n",
    "    \n",
    "    # Output the result\n",
    "    #print(selected_door_name)\n",
    "    for ob in bpy.data.objects:\n",
    "        # Check for \"door\" (case-insensitive)\n",
    "        if selected_door_name in ob.name:\n",
    "            global_loc = ob.matrix_world.translation\n",
    "            x_start=global_loc.x\n",
    "            y_start=global_loc.y\n",
    "\n",
    "    camera_data = bpy.context.scene.camera.data\n",
    "    if camera_data.type == 'PERSP':\n",
    "    \n",
    "        sensor_width = camera_data.sensor_width\n",
    "        sensor_height = camera_data.sensor_height\n",
    "        focal_length = camera_data.lens\n",
    "        hor_fov_rad = 2 * math.atan((sensor_width / 2) / focal_length)\n",
    "        ver_fov_rad = 2 * math.atan((sensor_height / 2) / focal_length)\n",
    "        #print(ver_fov_rad,hor_fov_rad)\n",
    "\n",
    "    #simplify 3d mesh in blender\n",
    "    target_poly_density=100000\n",
    "    org_poly_num=[]\n",
    "    simp_poly_num=[]\n",
    "    for obj in bpy.data.objects:\n",
    "        if obj.type == 'MESH' and \"spawn_asset\" in obj.name and all(x not in obj.name for x in exclude_obj):\n",
    "            cur_density=len(obj.data.polygons)/sum(p.area for p in obj.data.polygons)\n",
    "            org_poly_num.append(len(obj.data.polygons))\n",
    "            if cur_density>target_poly_density:\n",
    "                obj.select_set(True)\n",
    "                bpy.context.view_layer.objects.active = obj\n",
    "                \n",
    "                mod = obj.modifiers.new(name='Decimate', type='DECIMATE')\n",
    "                mod.ratio = target_poly_density/cur_density\n",
    "                mod.use_collapse_triangulate = True  # Optional, helps preserve shape\n",
    "                bpy.ops.object.mode_set(mode='OBJECT')\n",
    "                bpy.ops.object.modifier_apply(modifier=mod.name)\n",
    "            simp_poly_num.append(len(obj.data.polygons))\n",
    "            \n",
    "            #print(cur_density,obj.name)\n",
    "\n",
    "    trimesh_obj_list=[]\n",
    "    cnt=0\n",
    "    num_faces=[]\n",
    "    obj_name_list=[]\n",
    "    obj_dimensions_data = []\n",
    "    for obj in bpy.data.objects:\n",
    "        if obj.type == 'MESH' and \"spawn_asset\" in obj.name and all(x not in obj.name for x in exclude_obj):\n",
    "            \n",
    "            trimesh_obj=blender_mesh_to_trimesh(obj)\n",
    "            trimesh_obj_list.append(trimesh_obj)\n",
    "    \n",
    "            #num_verts.append(trimesh_obj.vertices.shape[0])\n",
    "            #org_num_faces.append(len(trimesh_obj.faces))\n",
    "            num_faces.append(len(trimesh_obj.faces))\n",
    "            \n",
    "            #print(f\"{cnt} Faces org: {len(trimesh_obj.faces)},{obj.name}\")\n",
    "            obj_name_list.append(obj.name)\n",
    "            cnt=cnt+1\n",
    "\n",
    "            dims = trimesh_obj.extents\n",
    "        \n",
    "            obj_dimensions_data.append({\n",
    "                \"name\": obj.name,\n",
    "                \"width_x\": dims[0],\n",
    "                \"length_y\": dims[1],\n",
    "                \"height_z\": dims[2]\n",
    "            })\n",
    "\n",
    "    csv_path = OUTPUT_DIR+\"/\"+\"object_bbox_dimensions.csv\"\n",
    "\n",
    "    try:\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            # Define headers\n",
    "            fieldnames = [\"name\", \"width_x\", \"length_y\", \"height_z\"]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            writer.writerows(obj_dimensions_data)\n",
    "            \n",
    "        print(f\"Successfully saved bounding box dimensions to: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save CSV: {e}\")\n",
    "            \n",
    "    # add wall\n",
    "    for obj in bpy.data.objects:\n",
    "        if \"dining-room_0/0.wall\" in obj.name and all(x not in obj.name for x in exclude_obj):\n",
    "            \n",
    "            trimesh_obj=blender_mesh_to_trimesh(obj)\n",
    "            trimesh_obj_list.append(trimesh_obj)\n",
    "\n",
    "    cnt=0\n",
    "    \n",
    "    polygons_2d=[]\n",
    "    \n",
    "    #exclude the wall\n",
    "    for mesh in trimesh_obj_list[:-1]:\n",
    "        cnt=cnt+1\n",
    "        start = time.time()\n",
    "        try:\n",
    "        # risky operation\n",
    "            polygons_2d.append(mesh_to_xy_polygon(mesh))\n",
    "        except Exception as e:\n",
    "            #print(f\"Skipping item due to error: {e}\")\n",
    "            continue  # skip to next item\n",
    "        end = time.time()\n",
    "        #print(f\"Elapsed time: {end - start:.4f} seconds\",mesh.vertices.shape)\n",
    "    \n",
    "    end_preprocessing = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    captured_obj_idx=[]\n",
    "    l_cam_x=[x_start]\n",
    "    l_cam_y=[y_start]\n",
    "    l_yaw=[math.pi/2]\n",
    "    l_pitch=[0]\n",
    "    num_obj=40\n",
    "    \n",
    "    max_sight_length=4\n",
    "    max_samplingcnt=2000\n",
    "    max_inside=0\n",
    "    x_resolution=480\n",
    "    min_dist=0.2\n",
    "    max_dist=2\n",
    "    \n",
    "    cur_cam_x=x_start\n",
    "    cur_cam_y=y_start\n",
    "    \n",
    "    l_cnt_occlusion_check=[]\n",
    "    l_max_percent_showed=[]\n",
    "    l_sampling_cnt=[]\n",
    "    l_cnt_inside_fov=[]\n",
    "    l_cnt_accessible=[]\n",
    "    #target_obj_index=[]\n",
    "    obj_end_time=[]\n",
    "    obj_start_time=[]\n",
    "\n",
    "    appearance_obj_list=[]\n",
    "    \n",
    "    for obj_cnt in range(len(obj_name_list)):\n",
    "        #print(\"object num\",obj_cnt )\n",
    "        max_percent_showed=0\n",
    "        cnt_occlusion_check=0\n",
    "        sampling_cnt=0\n",
    "        cnt_inside_fov=0\n",
    "        cnt_accessible=0\n",
    "        \n",
    "        ##### find closet obj\n",
    "    \n",
    "        distance2start=[]\n",
    "        for i in range( len(polygons_2d)):\n",
    "            if i not in captured_obj_idx:\n",
    "                center=polygons_2d[i].centroid\n",
    "                dis=((cur_cam_x - center.x)**2 + (cur_cam_y - center.y)**2) ** 0.5\n",
    "                distance2start.append(dis)\n",
    "            else:\n",
    "                distance2start.append(100)\n",
    "                \n",
    "        cloest = min(distance2start)\n",
    "        index_cloest = distance2start.index(cloest)\n",
    "        captured_obj_idx.append(index_cloest)\n",
    "        \n",
    "        ##### sample a viewpoint\n",
    "    \n",
    "        # scene loading for segmentation map, only need to load once for a target\n",
    "        scene = pyrender.Scene(bg_color=[0, 0, 0, 0], ambient_light=[1.0, 1.0, 1.0])\n",
    "        scene_target_obj = pyrender.Scene(bg_color=[0, 0, 0, 0], ambient_light=[1.0, 1.0, 1.0])\n",
    "        \n",
    "        for i, mesh in enumerate(trimesh_obj_list):  # mesh_list is your list of trimesh.Trimesh objects\n",
    "            color = get_unique_color(random.randint(0, (1 << 24) - 1))\n",
    "            mesh.visual.vertex_colors = np.tile(color, (len(mesh.vertices), 1))\n",
    "            pm = pyrender.Mesh.from_trimesh(mesh, smooth=False)\n",
    "            scene.add(pm)\n",
    "            if i==index_cloest:\n",
    "                scene_target_obj.add(pm)\n",
    "                target_color=color\n",
    "                #print(color)\n",
    "                \n",
    "        camera = pyrender.PerspectiveCamera(yfov=ver_fov_rad)\n",
    "        aspect_ratio = np.tan(hor_fov_rad / 2) / np.tan(ver_fov_rad / 2) # width / height\n",
    "        y_resolution=math.floor(x_resolution/aspect_ratio)\n",
    "        \n",
    "        default_camera_pose = np.array([\n",
    "            [1.0, 0.0, 0.0, 3],\n",
    "            [0.0, 1.0, 0.0, 12],\n",
    "            [0.0, 0.0, 1.0, 10],\n",
    "            [0.0, 0.0, 0.0, 1.0]\n",
    "        ])\n",
    "        camera_node=scene.add(camera, pose=default_camera_pose)\n",
    "        scene.set_pose(camera_node, pose=default_camera_pose)\n",
    "        camera_node_target_obj=scene_target_obj.add(camera, pose=default_camera_pose)\n",
    "        scene.set_pose(camera_node, pose=default_camera_pose)\n",
    "        \n",
    "    \n",
    "        obj_start_time.append(time.time())\n",
    "        #while (sampling_cnt<max_samplingcnt):\n",
    "        while (sampling_cnt<500):\n",
    "            if sampling_cnt%500==0:\n",
    "                print(\"###\")\n",
    "            sampling_cnt=sampling_cnt+1\n",
    "            dis2camera = random.uniform(min_dist, max_dist)\n",
    "            angle_random= random.uniform(0, math.pi)\n",
    "            yaw_random=random.uniform(math.pi/4, math.pi/4*3)\n",
    "            pitch_random=random.uniform(0, math.pi)\n",
    "    \n",
    "            obj_cur=polygons_2d[index_cloest]\n",
    "            new_camera_position= Point(obj_cur.centroid.x+dis2camera * math.cos(angle_random), obj_cur.centroid.y+dis2camera * math.sin(angle_random),height)\n",
    "            new_camera_position_2d= Point(obj_cur.centroid.x+dis2camera * math.cos(angle_random), obj_cur.centroid.y+dis2camera * math.sin(angle_random))\n",
    "            ##### if the position in accessible area\n",
    "            accessibility=True\n",
    "            if poly_room.buffer(-0.1).contains(new_camera_position_2d)==False:\n",
    "                accessibility=False\n",
    "            for o in polygons_2d:\n",
    "                if o.contains(new_camera_position_2d)==True:\n",
    "                    accessibility=False\n",
    "    \n",
    "            #print(\"accessibility\",accessibility)\n",
    "            if accessibility==True:\n",
    "                cnt_accessible=cnt_accessible+1\n",
    "                #print(\"accessibile\")\n",
    "                ##### if the target obj in fov\n",
    "                delta_z=max_sight_length*math.sin(ver_fov_rad/2)\n",
    "                delta_x=max_sight_length*math.cos(ver_fov_rad/2)*math.sin(hor_fov_rad/2)\n",
    "                delta_y=max_sight_length*math.cos(ver_fov_rad/2)*math.cos(hor_fov_rad/2)\n",
    "        \n",
    "                vertices = np.array([\n",
    "                    [new_camera_position.x, new_camera_position.y, height],   # cam\n",
    "                    [new_camera_position.x+delta_x, new_camera_position.y+delta_y, height+delta_z],   # fov 1\n",
    "                    [new_camera_position.x-delta_x, new_camera_position.y+delta_y, height+delta_z],   # fov 2\n",
    "                    [new_camera_position.x-delta_x, new_camera_position.y+delta_y, height-delta_z],\n",
    "                    [new_camera_position.x+delta_x, new_camera_position.y+delta_y, height-delta_z]\n",
    "                ])\n",
    "        \n",
    "                faces = np.array([\n",
    "                    [0, 1, 2],   # triangle \n",
    "                    [0, 2, 3],   # triangle \n",
    "                    [0, 3, 4],   # triangle \n",
    "                    [0, 4, 1],   # triangle \n",
    "                    [1, 2, 3],   # rec\n",
    "                    [1, 3, 4],   # rec\n",
    "                ])\n",
    "        \n",
    "                #fov without pitch and yaw\n",
    "                fov_mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=False)\n",
    "        \n",
    "                rotation_pitch = trimesh.transformations.rotation_matrix(\n",
    "                    pitch_random,               # angle in radians\n",
    "                    [0, 0, 1],               # axis of rotation (Z-axis)\n",
    "                    point=[new_camera_position.x, new_camera_position.y, height]     # rotate around the mesh center\n",
    "                )\n",
    "        \n",
    "                rotation_yaw = trimesh.transformations.rotation_matrix(\n",
    "                    yaw_random-math.pi/2,               # relative to current position\n",
    "                    [1, 0, 0],               # axis of rotation (X-axis)\n",
    "                    point=[new_camera_position.x, new_camera_position.y, height]     # rotate around the mesh center\n",
    "                )\n",
    "        \n",
    "                fov_mesh.apply_transform(rotation_yaw).apply_transform(rotation_pitch)\n",
    "        \n",
    "                \n",
    "                # Use mesh_b's vertices or sampled internal points\n",
    "                points = trimesh_obj_list[index_cloest].vertices  # or: mesh_b.sample(1000)\n",
    "                \n",
    "                # Check if all points are inside mesh_a\n",
    "                inside = fov_mesh.contains(points)\n",
    "                percent=np.mean(inside) * 100\n",
    "                if percent>max_inside:\n",
    "                    max_inside=percent\n",
    "                    #print(percent)\n",
    "                # True if all points of mesh_b are inside mesh_a\n",
    "                is_inside = inside.all()\n",
    "                \n",
    "                #print(is_inside)\n",
    "    \n",
    "                \n",
    "                if is_inside:\n",
    "                    cnt_inside_fov=cnt_inside_fov+1\n",
    "                    ##### check occlusion using segmentation map\n",
    "                    #print(\"###########\",sampling_cnt)\n",
    "                    cnt_occlusion_check=cnt_occlusion_check+1\n",
    "                    # Create a Rotation object from Euler angles (specify axes order, e.g. 'xyz' or 'zyx')\n",
    "                    r = R.from_euler('xyz', [yaw_random, 0, pitch_random])\n",
    "                    # Get the 3x3 rotation matrix\n",
    "                    R_matrix = r.as_matrix()\n",
    "                    # Define translation\n",
    "                    t = np.array([new_camera_position.x, new_camera_position.y, height])\n",
    "                    # Build the 4x4 transformation matrix\n",
    "                    T = np.eye(4)\n",
    "                    T[:3, :3] = R_matrix\n",
    "                    T[:3, 3] = t\n",
    "    \n",
    "    \n",
    "                    scene.set_pose(camera_node, pose=T)\n",
    "                    scene_target_obj.set_pose(camera_node_target_obj, pose=T)\n",
    "    \n",
    "    \n",
    "                    r = pyrender.OffscreenRenderer(viewport_width=x_resolution, viewport_height=y_resolution)\n",
    "                    color, _ = r.render(scene)\n",
    "                    color_target_obj, _ = r.render(scene_target_obj)\n",
    "                    plt.savefig(OUTPUT_DIR+\"/\"+ \"visual.pdf\")\n",
    "                    \n",
    "                    pixels = color_target_obj.reshape(-1, 3)\n",
    "                    # Get unique RGB rows\n",
    "                    unique_colors = np.unique(pixels, axis=0)\n",
    "                    # Optional: convert to list of tuples\n",
    "                    unique_color_list = [tuple(color) for color in unique_colors]\n",
    "                    target_color=unique_color_list[-1]\n",
    "                    \n",
    "                    mask = np.all(color == target_color, axis=-1)\n",
    "                    cnt_pixels_rendered=np.sum(mask)\n",
    "                    mask = np.all(color_target_obj == target_color, axis=-1)\n",
    "                    cnt_pixels_all=np.sum(mask)\n",
    "    \n",
    "                    percent_showed=cnt_pixels_rendered/cnt_pixels_all\n",
    "                    #print(\"percent showed\",percent_showed)\n",
    "                    if max_percent_showed<percent_showed:\n",
    "                        max_percent_showed=percent_showed\n",
    "                        #print(max_percent_showed)\n",
    "    \n",
    "                    if percent_showed>0.7 or cnt_occlusion_check>40:\n",
    "                        if percent_showed> 0.7:\n",
    "                            cur_cam_x=new_camera_position.x\n",
    "                            cur_cam_y=new_camera_position.y\n",
    "                            l_cam_x.append(new_camera_position.x)\n",
    "                            l_cam_y.append(new_camera_position.y)\n",
    "                            l_yaw.append(yaw_random)\n",
    "                            l_pitch.append(pitch_random)\n",
    "\n",
    "                            img = Image.fromarray(color)\n",
    "                            filename = f\"{OUTPUT_DIR}/segmentation_obj{obj_cnt}_all.png\"\n",
    "                            img.save(filename)\n",
    "\n",
    "                            img = Image.fromarray(color)\n",
    "                            filename = f\"{OUTPUT_DIR}/segmentation_obj{obj_cnt}_target.png\"\n",
    "                            img.save(filename)\n",
    "\n",
    "                            appearance_obj_list.append({\n",
    "                                \"name\": obj_name_list[index_cloest],\n",
    "                                \"occlusion\": percent_showed\n",
    "                            })\n",
    "\n",
    "                        break\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        ##### update\n",
    "        l_cnt_occlusion_check.append(cnt_occlusion_check)\n",
    "        l_max_percent_showed.append(max_percent_showed)\n",
    "        l_sampling_cnt.append(sampling_cnt)\n",
    "        l_cnt_inside_fov.append(cnt_inside_fov)\n",
    "        l_cnt_accessible.append(cnt_accessible)\n",
    "        obj_end_time.append(time.time())\n",
    "    \n",
    "    #print(\"finished\")\n",
    "\n",
    "    csv_path = OUTPUT_DIR+\"/\"+\"object_appearance.csv\"\n",
    "\n",
    "    try:\n",
    "        with open(csv_path, 'w', newline='') as f:\n",
    "            # Define headers\n",
    "            fieldnames = [\"name\", \"occlusion\"]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            writer.writerows(appearance_obj_list)\n",
    "            \n",
    "        print(f\"appearance: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save appearance\")\n",
    "\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. CONFIGURATION\n",
    "    # --------------------------------------------------------- # Resolution of the pathfinding grid (meters)\n",
    "    ROBOT_RADIUS = 0.1  # Buffer around obstacles so camera doesn't clip\n",
    "    \n",
    "    Z_HEIGHT = height # From your previous variable (1.5)\n",
    "    \n",
    "    # Combine obstacles into one collision object for faster checking\n",
    "    # We assume 'polygons_2d' contains your furniture footprints\n",
    "    obstacles_combined = unary_union(polygons_2d)\n",
    "    obstacles_buffered = obstacles_combined.buffer(ROBOT_RADIUS)\n",
    "    room_eroded = poly_room.buffer(-ROBOT_RADIUS) # Stay inside room\n",
    "    \n",
    "    # Prepare geometry for fast \"contains\" checks\n",
    "    prep_obstacles = prep(obstacles_buffered)\n",
    "    prep_room = prep(room_eroded)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 2. BUILD NAVIGATION GRAPH (Grid)\n",
    "    # ---------------------------------------------------------\n",
    "    #print(\"Building Navigation Graph...\")\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Get bounds of the room\n",
    "    minx, miny, maxx, maxy = poly_room.bounds\n",
    "    \n",
    "    # Generate grid nodes\n",
    "    x_coords = np.arange(minx, maxx, GRID_RES)\n",
    "    y_coords = np.arange(miny, maxy, GRID_RES)\n",
    "    \n",
    "    # Add nodes\n",
    "    valid_nodes = []\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            if is_valid_point(x, y,prep_room,prep_obstacles):\n",
    "                node_id = (round(x, 2), round(y, 2))\n",
    "                G.add_node(node_id)\n",
    "                valid_nodes.append(node_id)\n",
    "    \n",
    "    # Add edges (8-connectivity: horizontal, vertical, diagonal)\n",
    "    # We only add an edge if the connection doesn't cross an obstacle\n",
    "    directions = [\n",
    "        (GRID_RES, 0, 1.0),       # East\n",
    "        (-GRID_RES, 0, 1.0),      # West\n",
    "        (0, GRID_RES, 1.0),       # North\n",
    "        (0, -GRID_RES, 1.0),      # South\n",
    "        (GRID_RES, GRID_RES, 1.414),   # NE\n",
    "        (GRID_RES, -GRID_RES, 1.414),  # SE\n",
    "        (-GRID_RES, GRID_RES, 1.414),  # NW\n",
    "        (-GRID_RES, -GRID_RES, 1.414)  # SW\n",
    "    ]\n",
    "    \n",
    "    for node in valid_nodes:\n",
    "        x, y = node\n",
    "        for dx, dy, weight in directions:\n",
    "            neighbor = (round(x + dx, 2), round(y + dy, 2))\n",
    "            \n",
    "            if neighbor in G.nodes:\n",
    "                # Optional: Strict line-of-sight check for diagonal edges\n",
    "                # to prevent cutting through corners\n",
    "                # line = LineString([node, neighbor])\n",
    "                # if not prep_obstacles.intersects(line):\n",
    "                \n",
    "                # Simple distance weight\n",
    "                G.add_edge(node, neighbor, weight=weight)\n",
    "    \n",
    "    #print(f\"Graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4. GENERATE TRAJECTORY (TRANSLATE -> ROTATE)\n",
    "    # ---------------------------------------------------------\n",
    "    # full_trajectory will contain dictionaries: {'x', 'y', 'z', 'pitch', 'yaw', 'action'}\n",
    "    full_trajectory = []\n",
    "    \n",
    "    # Number of frames to interpolate rotation\n",
    "    \n",
    "    \n",
    "    for i in range(len(l_cam_x) - 1):\n",
    "        curr_x, curr_y = l_cam_x[i], l_cam_y[i]\n",
    "        next_x, next_y = l_cam_x[i+1], l_cam_y[i+1]\n",
    "        \n",
    "        curr_yaw, curr_pitch = l_yaw[i], l_pitch[i]\n",
    "        next_yaw, next_pitch = l_yaw[i+1], l_pitch[i+1]\n",
    "        \n",
    "        #print(f\"Planning segment {i}: ({curr_x:.1f}, {curr_y:.1f}) -> ({next_x:.1f}, {next_y:.1f})\")\n",
    "        \n",
    "        # A. TRANSLATION PHASE (Move along grid path)\n",
    "        # -------------------------------------------\n",
    "        path_points = compute_path_segment(curr_x, curr_y, next_x, next_y,G)\n",
    "        \n",
    "        # If path is empty (start==end), skip translation\n",
    "        if len(path_points) > 0:\n",
    "            for p in path_points:\n",
    "                full_trajectory.append({\n",
    "                    'x': p[0], 'y': p[1], 'z': Z_HEIGHT,\n",
    "                    'yaw': curr_yaw,   # Maintain previous orientation while moving\n",
    "                    'pitch': curr_pitch,\n",
    "                    'action': 'translate'\n",
    "                })\n",
    "                \n",
    "        # B. ROTATION PHASE (Stationary turn at destination)\n",
    "        # --------------------------------------------------\n",
    "        # Linearly interpolate angles from curr to next\n",
    "        # Note: Shortest path interpolation for angles (handle 0/360 wrap) is omitted for brevity \n",
    "        # but recommended for production.\n",
    "        for step in range(1, ROTATION_STEPS + 1):\n",
    "            alpha = step / ROTATION_STEPS\n",
    "            interp_yaw = curr_yaw + (next_yaw - curr_yaw) * alpha\n",
    "            interp_pitch = curr_pitch + (next_pitch - curr_pitch) * alpha\n",
    "            \n",
    "            full_trajectory.append({\n",
    "                'x': next_x, 'y': next_y, 'z': Z_HEIGHT, # Stay at dest\n",
    "                'yaw': interp_yaw,\n",
    "                'pitch': interp_pitch,\n",
    "                'action': 'rotate'\n",
    "            })\n",
    "    \n",
    "    #print(f\"Trajectory generated with {len(full_trajectory)} frames.\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 5. VISUALIZATION\n",
    "    # ---------------------------------------------------------\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plot Room\n",
    "    rx, ry = poly_room.exterior.xy\n",
    "    plt.plot(rx, ry, 'k-', linewidth=2, label='Room')\n",
    "    \n",
    "    # Plot Obstacles\n",
    "    for poly in polygons_2d:\n",
    "        if not poly.is_empty:\n",
    "            if poly.geom_type == 'Polygon':\n",
    "                ox, oy = poly.exterior.xy\n",
    "                plt.fill(ox, oy, color='gray', alpha=0.5)\n",
    "            elif poly.geom_type == 'MultiPolygon':\n",
    "                for sub_p in poly.geoms:\n",
    "                    ox, oy = sub_p.exterior.xy\n",
    "                    plt.fill(ox, oy, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Plot Trajectory\n",
    "    traj_x = [t['x'] for t in full_trajectory]\n",
    "    traj_y = [t['y'] for t in full_trajectory]\n",
    "    plt.plot(traj_x, traj_y, 'b.-', markersize=2, label='Computed Path')\n",
    "    \n",
    "    # Plot Keypoints\n",
    "    plt.plot(l_cam_x, l_cam_y, 'ro', markersize=8, label='Keypoints')\n",
    "    \n",
    "    plt.title(\"Dijkstra Path Planning (Translate -> Rotate)\")\n",
    "    plt.axis('equal')\n",
    "    plt.legend()\n",
    "   \n",
    "    #filepath = os.path.join(OUTPUT_DIR, \"visual.pdf\")\n",
    "    plt.savefig(OUTPUT_DIR+\"/\"+ \"visual.pdf\")\n",
    "    #plt.show()\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "    # 2. APPLY TO ANIMATION\n",
    "    # ---------------------------------------------------------\n",
    "    # Get the camera (Ensure we get the one from the outliner)\n",
    "    cam = bpy.data.objects.get(\"camera_0_0\") # Try specific name from your image\n",
    "    if not cam:\n",
    "        cam = bpy.context.scene.camera # Fallback to active\n",
    "\n",
    "\n",
    "    # Force Euler mode to ensure rotations apply correctly\n",
    "    if cam.rotation_mode != 'XYZ':\n",
    "        cam.rotation_mode = 'XYZ'\n",
    "    \n",
    "    # Clear any parent inverse correction issues or constraints if necessary\n",
    "    # (Optional: Only do this if the camera flies wildly off)\n",
    "    # if cam.parent:\n",
    "    #    cam.matrix_parent_inverse.identity()\n",
    "    \n",
    "    bpy.context.scene.frame_start = 0\n",
    "    bpy.context.scene.frame_end = len(full_trajectory) - 1\n",
    "\n",
    "    output_csv =OUTPUT_DIR+\"/\"+ \"trajectory_data.csv\"\n",
    "    \n",
    "    # Ensure there is data to save\n",
    "    if len(full_trajectory) > 0:\n",
    "        # Get column headers from the first dictionary keys\n",
    "        keys = full_trajectory[0].keys()\n",
    "    \n",
    "        with open(output_csv, 'w', newline='') as f:\n",
    "            dict_writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(full_trajectory)\n",
    "    \n",
    "        print(f\"Successfully saved {len(full_trajectory)} rows to {os.path.abspath(output_csv)}\")\n",
    "    else:\n",
    "        print(\"Trajectory is empty, nothing to save.\")\n",
    "    \n",
    "    for i, state in enumerate(full_trajectory):\n",
    "        bpy.context.scene.frame_set(i)\n",
    "        \n",
    "        # Unpack trajectory\n",
    "        x, y, z = state['x'], state['y'], state['z']\n",
    "        yaw, pitch = state['yaw'], state['pitch']\n",
    "        \"\"\"\n",
    "        # 1. POSITION FIX: Use matrix_world to ignore parent offsets\n",
    "        target_pos = mathutils.Vector((x, y, z))\n",
    "        \n",
    "        # 2. ROTATION FIX: Calculate correct matrix for Blender Camera\n",
    "        # Note: We reconstruct the matrix entirely to ensure correct orientation\n",
    "        new_matrix = get_camera_rotation_matrix(target_pos, yaw, pitch)\n",
    "        \n",
    "        # Apply to camera\n",
    "        cam.matrix_world = new_matrix\n",
    "        \"\"\"\n",
    "        # 1. ROTATION: Match PyRender Logic (Euler XYZ)\n",
    "        # This ensures the render matches your segmentation maps\n",
    "        rot_euler = mathutils.Euler((yaw, 0, pitch), 'XYZ')\n",
    "        \n",
    "        # 2. CONSTRUCT MATRIX WORLD\n",
    "        mat_rot = rot_euler.to_matrix().to_4x4()\n",
    "        mat_loc = mathutils.Matrix.Translation((x, y, z))\n",
    "        \n",
    "        # Combine and Apply\n",
    "        cam.matrix_world = mat_loc @ mat_rot\n",
    "        # Keyframe Matrix (VisualLocRot handles the matrix decomposition automatically)\n",
    "        cam.keyframe_insert(data_path=\"location\", index=-1)\n",
    "        cam.keyframe_insert(data_path=\"rotation_euler\", index=-1)\n",
    "\n",
    "    \n",
    "    #print(\"Camera trajectory corrected using World Matrix.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_path_abs = OUTPUT_DIR\n",
    "    if not os.path.exists(output_path_abs):\n",
    "        os.makedirs(output_path_abs)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 2. SETUP CYCLES RENDERER\n",
    "    # ---------------------------------------------------------\n",
    "    scene = bpy.context.scene\n",
    "    scene.render.engine = 'CYCLES'\n",
    "    \n",
    "    # Set Device (GPU if available, else CPU)\n",
    "    prefs = bpy.context.preferences.addons['cycles'].preferences\n",
    "    try:\n",
    "        # Try to set CUDA or OPTIX if available\n",
    "        prefs.compute_device_type = 'CUDA' \n",
    "        prefs.get_devices()\n",
    "    except:\n",
    "        pass\n",
    "    scene.cycles.device = 'GPU'\n",
    "    \n",
    "    # Apply User Settings\n",
    "    scene.cycles.samples = 64\n",
    "    scene.cycles.use_adaptive_sampling = True\n",
    "    scene.cycles.adaptive_threshold = 0.01\n",
    "    scene.cycles.use_denoising = True\n",
    "    scene.view_settings.exposure = 1.0\n",
    "    \n",
    "    # Resolution (optional, matches your notebook settings if needed)\n",
    "    scene.render.resolution_x = 1920\n",
    "    scene.render.resolution_y = 1080\n",
    "    scene.render.resolution_percentage = 100\n",
    "\n",
    "    \n",
    "    scene.frame_step = 3\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 3. RENDER FRAMES (PNG SEQUENCE)\n",
    "    # ---------------------------------------------------------\n",
    "    #print(\"Starting Frame Render...\")\n",
    "    \n",
    "    # Set output format to PNG\n",
    "    scene.render.image_settings.file_format = 'PNG'\n",
    "    scene.render.filepath = os.path.join(OUTPUT_DIR, FRAME_FILENAME)\n",
    "    \n",
    "    # Render the animation\n",
    "    # This saves frame_0000.png, frame_0001.png, etc.\n",
    "    bpy.ops.render.render(animation=True)\n",
    "    \n",
    "    #print(\"Frames rendered successfully.\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4. GENERATE VIDEO (FROM FRAMES)\n",
    "    # ---------------------------------------------------------\n",
    "    #print(\"Compiling Video...\")\n",
    "    \n",
    "    # Switch to Video Sequence Editor\n",
    "    \"\"\"\n",
    "    if not scene.sequence_editor:\n",
    "        scene.sequence_editor_create()\n",
    "        \n",
    "    seq = scene.sequence_editor.sequences\n",
    "    \n",
    "    # Clear existing strips\n",
    "    for s in list(seq):\n",
    "        seq.remove(s)\n",
    "    \n",
    "    # Add the rendered image sequence to the VSE\n",
    "    frame_start = scene.frame_start\n",
    "    frame_end = scene.frame_end\n",
    "    filenames = [f\"{FRAME_FILENAME}{i:04d}.png\" for i in range(frame_start, frame_end + 1)]\n",
    "    \n",
    "    # Import images as a strip\n",
    "    # Note: We assume the files were just created in output_path_abs\n",
    "    files = [{\"name\": fname} for fname in filenames]\n",
    "    strip = seq.new_image(\n",
    "        name=\"RenderedStrip\",\n",
    "        filepath=os.path.join(output_path_abs, filenames[0]),\n",
    "        channel=1,\n",
    "        frame_start=frame_start\n",
    "    )\n",
    "    \n",
    "    # Important: add all files to the strip\n",
    "    for f in filenames:\n",
    "        strip.elements.append(f)\n",
    "    \n",
    "    # Change Render Settings for Video Output\n",
    "    scene.render.image_settings.file_format = 'FFMPEG'\n",
    "    scene.render.ffmpeg.format = 'MPEG4'\n",
    "    scene.render.ffmpeg.codec = 'H264'\n",
    "    scene.render.ffmpeg.constant_rate_factor = 'MEDIUM'  # Quality\n",
    "    scene.render.filepath = os.path.join(OUTPUT_DIR, VIDEO_FILENAME)\n",
    "    \n",
    "    # Render the video file (using the sequencer we just set up)\n",
    "    # We use 'write_still=False' because we are rendering a video file now\n",
    "    bpy.ops.render.render(animation=True)\n",
    "    \n",
    "    # Restore settings (optional)\n",
    "    scene.render.image_settings.file_format = 'PNG'\n",
    "    scene.render.filepath = os.path.join(OUTPUT_DIR, FRAME_FILENAME)\n",
    "    \n",
    "    #print(f\"Video saved to {os.path.join(output_path_abs, VIDEO_FILENAME)}\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf4ed1-5cac-4b4b-bf61-12d16d92c1fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4345f-8e60-443c-b9f2-71ed373b7481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74affa8-e2c2-4b30-ae47-71c097b2231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "\n",
      "G:\\backup\\living\\scene (1).blend\n",
      "Warning: File written by newer Blender binary (404.32), expect loss of data!\n",
      "Successfully saved bounding box dimensions to: G:/backup/living/scene (1)/object_bbox_dimensions.csv\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "appearance: G:/backup/living/scene (1)/object_appearance.csv\n",
      "No path found between (8.870227770976326,14.143349726026141) and (4.3158613981551355,8.23600809031156)\n",
      "Successfully saved 276 rows to G:\\backup\\living\\scene (1)\\trajectory_data.csv\n",
      "##################################################\n",
      "\n",
      "G:\\backup\\living\\scene (10).blend\n",
      "Warning: File written by newer Blender binary (404.32), expect loss of data!\n",
      "Successfully saved bounding box dimensions to: G:/backup/living/scene (10)/object_bbox_dimensions.csv\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the path using 'r' (raw string) to handle backslashes correctly\n",
    "folder_path = Path(r\"G:\\backup\\living\")\n",
    "\n",
    "# 2. Use glob to find all files ending in .blend\n",
    "# This creates a generator of Path objects\n",
    "blend_files = folder_path.glob(\"*.blend\")\n",
    "\n",
    "# 3. Print or process them\n",
    "for file in blend_files:\n",
    "    file_name=str(file)\n",
    "    room_type=\"living-room_0/0.ceiling\"\n",
    "    OUTPUT_DIR = \"G:/backup/living\"+\"/\" + file.name  # Relative path (creates folder next to .blend file)\n",
    "    OUTPUT_DIR=OUTPUT_DIR[:-6]\n",
    "    FRAME_FILENAME = \"frame_\"\n",
    "    VIDEO_FILENAME = \"trajectory_video.mp4\"\n",
    "    GRID_RES = 0.1\n",
    "    ROTATION_STEPS = 3\n",
    "    height=1.5\n",
    "        \n",
    "    print(\"##################################################\\n\")\n",
    "    print(file_name)\n",
    "    folder_path = Path(OUTPUT_DIR)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    camera_traj(file_name,\n",
    "                room_type,\n",
    "                OUTPUT_DIR,\n",
    "                FRAME_FILENAME,\n",
    "                VIDEO_FILENAME,\n",
    "                GRID_RES,\n",
    "                ROTATION_STEPS,\n",
    "                height\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dcd0f7-1adf-4ec3-8d40-10db79c3644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"G:/backup/living\"+\"/\" + file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d81b2-b710-4819-bf90-5421422e60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(OUTPUT_DIR[:-6])\n",
    "folder_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1061a6-4a8f-4531-ad70-9f3e27d7c699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
